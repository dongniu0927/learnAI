{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN演变史"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **LeNet-5**\n",
    "\n",
    "Yann LeCun在1998年提出，它奠定了整个卷积神经网络的基础。卷积网络结合了3种关键性思想: **局部感受野(稀疏连接)**, **权重共享**, **子空间采样**. 这3种思想确保了模型对图像的平移、缩放和扭曲具有一定程度的不变性。\n",
    "\n",
    "其中**局部感受野表示卷积核只关注图像的局部特征**，而**权重共享表示一个卷积核在整张图像上都使用相同的权值**，最后的**子采样即我们常用的池化操作，它可以精炼抽取的特征**。\n",
    "\n",
    "LeCun提出CNN后，一些表现更好的手工设计的特征比如：SIFT,HOG,LBP这些算子再加上SVM，原理的清晰性，模型可解释，数据需求小，计算需求小，很快盖过了LeCun的工作, CNN陷入低潮.\n",
    "\n",
    "> Y. LeCun, L. Bottou, Y. Bengio and P. Haffner: Gradient-Based Learning Applied to Document Recognition, Proceedings of the IEEE, 86(11):2278-2324, November 1998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **AlexNet**\n",
    "\n",
    "Sutskever与Hinton在2012年提出，使用了更深的CNN并使用GPU并行计算。这一次工作直接**导致了神经网络的复兴**，也采用了许多技巧，比如dropout，relu激活函数，学习率衰减等。\n",
    "\n",
    "> Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[C]// International Conference on Neural Information Processing Systems. Curran Associates Inc. 2012:1097-1105."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **VGG-net**\n",
    "\n",
    "牛津大学在2014年提出了另一种深度卷积网络，它相比于 AlexNet 有更小的卷积核和更深的层级。VGG-Net 的泛化性能较好，常用于图像特征的抽取目标检测候选框生成等。其最主要的意义就是实践了**神经网络越深越好**的理念。\n",
    "\n",
    "> Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. **GoogleNet (Iception V1)**\n",
    "\n",
    "Google在2014年提出，其核心的思想就是：稀疏连接。目的在于解决因为想提升网络性能，增加层数，而让参数变多引起的过拟合与计算量的增加。Inception结构的原理实际上就是**将不同尺寸的卷积组合起来，以提供不同尺寸的特征**。\n",
    "\n",
    "> Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[J]. 2014:1-9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. **ResNet**\n",
    "\n",
    "何凯明等人于2015年提出，其骤然将网络深度由十几二十层提升到上百层。ResNet 最大的特点即**引入残差单元解决了反向传播过程中的梯度消失问题**，因此它可以训练非常深的网络而不用像 GoogLeNet那样在中间添加分类网络以提供额外的梯度。\n",
    "\n",
    "> He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. **DenseNet**\n",
    "\n",
    "Gao Huang等研究者提出了DenseNet，它的目标是**提升网络层级间信息流与梯度流的效率**，并提高参数效率。该论文获得了 CVPR 2017 的最佳论文。\n",
    "\n",
    "> Huang G, Liu Z, Laurens V D M, et al. Densely Connected Convolutional Networks[J]. 2016:2261-2269."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "- [深度学习（六）——CNN进化史](https://blog.csdn.net/antkillerfarm/article/details/77834124)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
