{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 中的优化器\n",
    "\n",
    "> 这里主要介绍基于梯度下降的优化方法及其变种\n",
    "\n",
    "## 梯度下降\n",
    "- 全批量梯度下降\n",
    "  - $\\theta = \\theta - \\eta \\cdot \\nabla _ { \\theta } J ( \\theta )$\n",
    "  - 计算所有样本的总误差，根据总误差更新权值\n",
    "  - 在大数据集上训练速度很慢，可能包含冗余数据\n",
    "  \n",
    "- 随机梯度下降(SGD, stochastic gradient descent)\n",
    "  - $\\theta = \\theta - \\eta \\cdot \\nabla _ { \\theta } J \\left( \\theta ; x ^ { ( i ) } ; y ^ { ( i ) } \\right)$\n",
    "  - 是一种使用批量(batch)的方法，本质上是喂数据的策略不同，一次使用一个样本来进行计算并更新迭代, 批量大小为1\n",
    "  - 更新过程会有大量噪声，每次迭代不一定是向减少损失的方向前进 \n",
    "\n",
    "- 小批量梯度下降(mini-batch SGD)\n",
    "  - $\\theta = \\theta - \\eta \\cdot \\nabla _ { \\theta } J \\left( \\theta ; x ^ { ( i : i + n ) } ; y ^ { ( i : i + n ) } \\right)$\n",
    "  - 是标准和随机梯度下降方法的折中，小批量通常包含10-1000个随机选择的样本\n",
    "  - 可以减少SGD过程中杂乱样本数量，比全批量更高效\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# <---! def common variables !--->\n",
    "y = tf.constant(3, dtype=tf.float32)\n",
    "x = tf.placeholder(dtype=tf.float32)\n",
    "w = tf.Variable(2, dtype=tf.float32)\n",
    "\n",
    "# <---! def common operations !--->\n",
    "p = w * x\n",
    "# Losses\n",
    "losses = tf.square(p - y)\n",
    "# Gradients\n",
    "grad = tf.gradients(losses, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- Test gradient ----------------------------------------\n",
      "[[-2.0], 2.0, 2.0]\n",
      "Iteration: 0, w:2.0, g: [-2.0], loss: 1.0\n",
      "Iteration: 1, w:2.5999999046325684, g: [-0.8000002], loss: 0.16000007092952728\n",
      "Iteration: 2, w:2.8399999141693115, g: [-0.32000017], loss: 0.025600027292966843\n",
      "Iteration: 3, w:2.935999870300293, g: [-0.12800026], loss: 0.004096016753464937\n",
      "Iteration: 4, w:2.974400043487549, g: [-0.051199913], loss: 0.0006553577841259539\n",
      "Iteration: 5, w:2.989759922027588, g: [-0.020480156], loss: 0.00010485919483471662\n",
      "Iteration: 6, w:2.995903968811035, g: [-0.008192062], loss: 1.677747059147805e-05\n",
      "Iteration: 7, w:2.998361587524414, g: [-0.003276825], loss: 2.6843954401556402e-06\n",
      "Iteration: 8, w:2.99934458732605, g: [-0.0013108253], loss: 4.2956577317454503e-07\n",
      "Iteration: 9, w:2.9997377395629883, g: [-0.0005245209], loss: 6.87805368215777e-08\n",
      "Iteration: 10, w:2.9998950958251953, g: [-0.00020980835], loss: 1.1004885891452432e-08\n",
      "Iteration: 11, w:2.999958038330078, g: [-8.392334e-05], loss: 1.760781742632389e-09\n",
      "Iteration: 12, w:2.999983310699463, g: [-3.33786e-05], loss: 2.7853275241795927e-10\n",
      "Iteration: 13, w:2.999993324279785, g: [-1.335144e-05], loss: 4.4565240386873484e-11\n",
      "Iteration: 14, w:2.99999737739563, g: [-5.2452087e-06], loss: 6.87805368215777e-12\n",
      "Iteration: 15, w:2.9999990463256836, g: [-1.9073486e-06], loss: 9.094947017729282e-13\n",
      "Iteration: 16, w:2.999999523162842, g: [-9.536743e-07], loss: 2.2737367544323206e-13\n",
      "Iteration: 17, w:2.999999761581421, g: [-4.7683716e-07], loss: 5.684341886080802e-14\n",
      "Iteration: 18, w:3.0, g: [0.0], loss: 0.0\n",
      "Iteration: 19, w:3.0, g: [0.0], loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "def test_gradient():\n",
    "    global losses, w, grad\n",
    "    # Learning rate\n",
    "    lr = tf.constant(0.3, dtype=tf.float32)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    # Update the value of w\n",
    "    update = tf.assign(w, w - lr * grad[0])\n",
    "    with tf.Session() as sess:\n",
    "        print('--' * 20, 'Test gradient', '--' * 20)\n",
    "        sess.run(init)\n",
    "        print(sess.run([grad, p, w], {x: 1}))\n",
    "\n",
    "        for i in range(20):\n",
    "            w_, g_, l_ = sess.run([w, grad, losses], feed_dict={x: 1})\n",
    "            print('Iteration: {}, w:{}, g: {}, loss: {}'.format(i, w_, g_, l_))\n",
    "\n",
    "            _ = sess.run(update, feed_dict={x: 1})\n",
    "test_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动量优化器(Momentum)\n",
    "- 不使用momentum\n",
    "![sgd_without_momentum](./resources/sgd_without_momentum.gif)\n",
    "SGD随机梯度下降在更新的过程中会出现更大扰动，这种情况常常出现在局部最优点。\n",
    "\n",
    "\n",
    "- 使用momentum\n",
    "  - $\\begin{aligned} v _ { t } & = \\gamma v _ { t - 1 } + \\eta \\nabla _ { \\theta } J ( \\theta ) \\\\ \\theta & = \\theta - v _ { t } \\end{aligned}$\n",
    "![sgd_with_momentum](./resources/sgd_with_momentum.gif)\n",
    "  - Momentum动量帮助随机梯度下降在相关的帮助减少损失的方向加速，而抑制扰动。\n",
    "  - 我们为momentum添加了一个分数参数$\\gamma$，这个值通常为0.9或近似值，使得SGD在正确的方向上下降得更快。\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试Momentum优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- Test momentum api ----------------------------------------\n",
      "[[-2.0], 2.0, 2.0]\n",
      "Iteration: 0, w:2.0, g: [-2.0], loss: 1.0\n",
      "Iteration: 1, w:2.5999999046325684, g: [-0.8000002], loss: 0.16000007092952728\n",
      "Iteration: 2, w:3.380000114440918, g: [0.7600002], loss: 0.14440008997917175\n",
      "Iteration: 3, w:3.8540000915527344, g: [1.7080002], loss: 0.7293161749839783\n",
      "Iteration: 4, w:3.768199920654297, g: [1.5363998], loss: 0.5901311039924622\n",
      "Iteration: 5, w:3.230059862136841, g: [0.46011972], loss: 0.05292753875255585\n",
      "Iteration: 6, w:2.6076979637145996, g: [-0.7846041], loss: 0.1539008915424347\n",
      "Iteration: 7, w:2.2829535007476807, g: [-1.434093], loss: 0.5141556859016418\n",
      "Iteration: 8, w:2.4209113121032715, g: [-1.1581774], loss: 0.33534371852874756\n",
      "Iteration: 9, w:2.892526626586914, g: [-0.21494675], loss: 0.01155052613466978\n",
      "Iteration: 10, w:3.3814644813537598, g: [0.76292896], loss: 0.14551514387130737\n",
      "Iteration: 11, w:3.592629909515381, g: [1.1852598], loss: 0.35121020674705505\n",
      "Iteration: 12, w:3.427100658416748, g: [0.8542013], loss: 0.1824149787425995\n",
      "Iteration: 13, w:3.0218639373779297, g: [0.043727875], loss: 0.0004780317540280521\n",
      "Iteration: 14, w:2.6440324783325195, g: [-0.71193504], loss: 0.1267128735780716\n",
      "Iteration: 15, w:2.5175647735595703, g: [-0.96487045], loss: 0.2327437549829483\n",
      "Iteration: 16, w:2.693204879760742, g: [-0.61359024], loss: 0.0941232442855835\n",
      "Iteration: 17, w:3.035358190536499, g: [0.07071638], loss: 0.001250201603397727\n",
      "Iteration: 18, w:3.3220810890197754, g: [0.6441622], loss: 0.10373622924089432\n",
      "Iteration: 19, w:3.386883020401001, g: [0.77376604], loss: 0.14967846870422363\n"
     ]
    }
   ],
   "source": [
    "def test_momentum_api():\n",
    "    global losses, w, grad\n",
    "\n",
    "    # momentum\n",
    "    mu = 0.9\n",
    "    lr = tf.constant(0.03, dtype=tf.float32)\n",
    "    # init = tf.global_variables_initializer()\n",
    "\n",
    "    # Update the var list to optimize losses\n",
    "    update = tf.train.MomentumOptimizer(lr, mu).minimize(losses)\n",
    "    with tf.Session() as sess:\n",
    "        print('--' * 20, 'Test momentum api', '--' * 20)\n",
    "        # sess.run(init)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(sess.run([grad, p, w], {x: 1}))\n",
    "        for i in range(20):\n",
    "            w_, g_, l_ = sess.run([w, grad, losses], feed_dict={x: 1})\n",
    "            print('Iteration: {}, w:{}, g: {}, loss: {}'.format(i, w_, g_, l_))\n",
    "\n",
    "            _ = sess.run([update], feed_dict={x: 1})\n",
    "\n",
    "test_momentum_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- Test momentum hand ----------------------------------------\n",
      "[[-2.0], 2.0, 2.0]\n",
      "Iteration: 0, w:2.0, g: [-2.0], loss: 1.0, v: 0.0\n",
      "Iteration: 1, w:2.059999942779541, g: [-1.8800001], loss: 0.883600115776062, v: -0.05999999865889549\n",
      "Iteration: 2, w:2.1703999042510986, g: [-1.6592002], loss: 0.6882362961769104, v: -0.1103999987244606\n",
      "Iteration: 3, w:2.319535970687866, g: [-1.360928], loss: 0.4630312919616699, v: -0.1491360068321228\n",
      "Iteration: 4, w:2.494586229324341, g: [-1.0108275], loss: 0.2554430663585663, v: -0.17505024373531342\n",
      "Iteration: 5, w:2.6824562549591064, g: [-0.6350875], loss: 0.10083402693271637, v: -0.18787004053592682\n",
      "Iteration: 6, w:2.870591878890991, g: [-0.25881624], loss: 0.016746461391448975, v: -0.18813565373420715\n",
      "Iteration: 7, w:3.0476784706115723, g: [0.09535694], loss: 0.002273236634209752, v: -0.17708657681941986\n",
      "Iteration: 8, w:3.204195737838745, g: [0.40839148], loss: 0.04169590026140213, v: -0.15651720762252808\n",
      "Iteration: 9, w:3.3328094482421875, g: [0.6656189], loss: 0.11076212674379349, v: -0.12861374020576477\n",
      "Iteration: 10, w:3.428593158721924, g: [0.8571863], loss: 0.1836920976638794, v: -0.09578379988670349\n",
      "Iteration: 11, w:3.4890830516815186, g: [0.9781661], loss: 0.23920223116874695, v: -0.060489825904369354\n",
      "Iteration: 12, w:3.514178991317749, g: [1.028358], loss: 0.2643800377845764, v: -0.025095859542489052\n",
      "Iteration: 13, w:3.5059144496917725, g: [1.0118289], loss: 0.2559494376182556, v: 0.008264465257525444\n",
      "Iteration: 14, w:3.4681215286254883, g: [0.93624306], loss: 0.2191377729177475, v: 0.037792883813381195\n",
      "Iteration: 15, w:3.4060206413269043, g: [0.8120413], loss: 0.16485276818275452, v: 0.062100887298583984\n",
      "Iteration: 16, w:3.3257687091827393, g: [0.6515374], loss: 0.10612525045871735, v: 0.0802520364522934\n",
      "Iteration: 17, w:3.2339956760406494, g: [0.46799135], loss: 0.05475397780537605, v: 0.09177295118570328\n",
      "Iteration: 18, w:3.1373603343963623, g: [0.27472067], loss: 0.018867861479520798, v: 0.09663539379835129\n",
      "Iteration: 19, w:3.042146921157837, g: [0.08429384], loss: 0.0017763630021363497, v: 0.09521347284317017\n"
     ]
    }
   ],
   "source": [
    "def test_momentum_hand():\n",
    "    global losses, w, grad\n",
    "    mu = 0.9\n",
    "    lr = tf.constant(0.03, dtype=tf.float32)\n",
    "    v = tf.Variable(0, dtype=tf.float32)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # update\n",
    "    update1 = tf.assign(v, mu * v + grad[0] * lr)\n",
    "    update2 = tf.assign(w, w - v)\n",
    "    with tf.Session() as sess:\n",
    "        print('--' * 20, 'Test momentum hand', '--' * 20)\n",
    "        sess.run(init)\n",
    "        print(sess.run([grad, p, w], {x: 1}))\n",
    "        for i in range(20):\n",
    "            w_, g_, l_, v_ = sess.run([w, grad, losses, v], feed_dict={x: 1})\n",
    "            print('Iteration: {}, w:{}, g: {}, loss: {}, v: {}'.format(i, w_, g_, l_, v_))\n",
    "            _ = sess.run([update1], feed_dict={x: 1})\n",
    "            _ = sess.run([update2], feed_dict={x: 1})\n",
    "\n",
    "test_momentum_hand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad优化器\n",
    "Adagrad使学习率适应参数，为那些经常出现的特征执行更小的更新(低学习率)，为不常出现的特征执行更大的更新(高学习率)，因此它很适合用以处理稀疏数据。\n",
    "\n",
    "### 更新过程\n",
    "$g _ { t , i } = \\nabla _ { \\theta } J \\left( \\theta _ { t , i } \\right)$\n",
    "\n",
    "$g_{t,i}$表示t时刻对$\\theta_i$的偏导\n",
    "\n",
    "$\\theta _ { t + 1 } = \\theta _ { t } - \\frac { \\eta } { \\sqrt { G _ { t } + \\epsilon } } \\odot g _ { t }$\n",
    "\n",
    "$G _ { t } \\in \\mathbb { R } ^ { d \\times d }$是对角矩阵，为$\\theta_{t,i}$的平方，$\\epsilon$是平滑参数，避免分母为0，常取值为`1e-8`\n",
    "\n",
    "### 优点和缺点\n",
    "- **优点**: 不需要手动调节学习率，通常默认0.01\n",
    "- **缺点**: 最大的缺点就是在分母里不断累加的平方项，可能会使得最后的学习率很小，可以通过Adadelta方法来解决。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- Test adagrad api ----------------------------------------\n",
      "[[-2.0], 2.0, 2.0]\n",
      "Iteration: 0, w:2.0, g: [-2.0], loss: 1.0\n",
      "Iteration: 1, w:2.592637777328491, g: [-0.81472445], loss: 0.16594398021697998\n",
      "Iteration: 2, w:2.816606044769287, g: [-0.3667879], loss: 0.03363334387540817\n",
      "Iteration: 3, w:2.916041851043701, g: [-0.1679163], loss: 0.007048970554023981\n",
      "Iteration: 4, w:2.9614334106445312, g: [-0.07713318], loss: 0.0014873817563056946\n",
      "Iteration: 5, w:2.982271671295166, g: [-0.035456657], loss: 0.00031429363298229873\n",
      "Iteration: 6, w:2.991849422454834, g: [-0.016301155], loss: 6.64319159113802e-05\n",
      "Iteration: 7, w:2.9962525367736816, g: [-0.0074949265], loss: 1.4043480405234732e-05\n",
      "Iteration: 8, w:2.998276948928833, g: [-0.0034461021], loss: 2.9689049370063003e-06\n",
      "Iteration: 9, w:2.9992077350616455, g: [-0.0015845299], loss: 6.276837325458473e-07\n",
      "Iteration: 10, w:2.999635696411133, g: [-0.0007286072], loss: 1.3271710486151278e-07\n",
      "Iteration: 11, w:2.9998323917388916, g: [-0.00033521652], loss: 2.8092529191781068e-08\n",
      "Iteration: 12, w:2.99992299079895, g: [-0.0001540184], loss: 5.930417046329239e-09\n",
      "Iteration: 13, w:2.999964475631714, g: [-7.104874e-05], loss: 1.2619807421287987e-09\n",
      "Iteration: 14, w:2.999983549118042, g: [-3.2901764e-05], loss: 2.7063151719630696e-10\n",
      "Iteration: 15, w:2.9999923706054688, g: [-1.5258789e-05], loss: 5.820766091346741e-11\n",
      "Iteration: 16, w:2.9999964237213135, g: [-7.1525574e-06], loss: 1.2789769243681803e-11\n",
      "Iteration: 17, w:2.9999983310699463, g: [-3.33786e-06], loss: 2.7853275241795927e-12\n",
      "Iteration: 18, w:2.9999992847442627, g: [-1.4305115e-06], loss: 5.115907697472721e-13\n",
      "Iteration: 19, w:2.999999761581421, g: [-4.7683716e-07], loss: 5.684341886080802e-14\n"
     ]
    }
   ],
   "source": [
    "def test_adagrad_api():\n",
    "    global losses, w, grad\n",
    "    lr = tf.constant(0.6, dtype=tf.float32)\n",
    "    update = tf.train.AdagradOptimizer(lr).minimize(losses)\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        print('--' * 20, 'Test adagrad api', '--' * 20)\n",
    "        sess.run(init)\n",
    "        print(sess.run([grad, p, w], {x: 1}))\n",
    "        for i in range(20):\n",
    "            w_, g_, l_ = sess.run([w, grad, losses], feed_dict={x: 1})\n",
    "            print('Iteration: {}, w:{}, g: {}, loss: {}'.format(i, w_, g_, l_))\n",
    "            _ = sess.run([update], feed_dict={x: 1})\n",
    "            \n",
    "test_adagrad_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- Test adagrad hand ----------------------------------------\n",
      "[[-2.0], 2.0, 2.0]\n",
      "Iteration: 0, w:2.0, g: [-2.0], loss: 1.0, sd:4.0\n",
      "Iteration: 1, w:2.5999999046325684, g: [-0.8000002], loss: 0.16000007092952728, sd:4.640000343322754\n",
      "Iteration: 2, w:2.8228342533111572, g: [-0.3543315], loss: 0.031387701630592346, sd:4.7655510902404785\n",
      "Iteration: 3, w:2.920222043991089, g: [-0.15955591], loss: 0.006364522036164999, sd:4.791008949279785\n",
      "Iteration: 4, w:2.963959217071533, g: [-0.072081566], loss: 0.0012989380629733205, sd:4.796204566955566\n",
      "Iteration: 5, w:2.9837074279785156, g: [-0.032585144], loss: 0.0002654478885233402, sd:4.797266483306885\n",
      "Iteration: 6, w:2.992633819580078, g: [-0.014732361], loss: 5.426061397884041e-05, sd:4.797483444213867\n",
      "Iteration: 7, w:2.9966695308685303, g: [-0.0066609383], loss: 1.1092024578829296e-05, sd:4.79752779006958\n",
      "Iteration: 8, w:2.9984941482543945, g: [-0.0030117035], loss: 2.2675894797430374e-06, sd:4.797536849975586\n",
      "Iteration: 9, w:2.999319076538086, g: [-0.0013618469], loss: 4.636567609850317e-07, sd:4.797538757324219\n",
      "Iteration: 10, w:2.99969220161438, g: [-0.0006155968], loss: 9.473984619035036e-08, sd:4.797539234161377\n",
      "Iteration: 11, w:2.9998607635498047, g: [-0.0002784729], loss: 1.9386789062991738e-08, sd:4.797539234161377\n",
      "Iteration: 12, w:2.999937057495117, g: [-0.00012588501], loss: 3.961758920922875e-09, sd:4.797539234161377\n",
      "Iteration: 13, w:2.999971628189087, g: [-5.6743622e-05], loss: 8.049596544879023e-10, sd:4.797539234161377\n",
      "Iteration: 14, w:2.9999871253967285, g: [-2.5749207e-05], loss: 1.6575540939811617e-10, sd:4.797539234161377\n",
      "Iteration: 15, w:2.9999942779541016, g: [-1.1444092e-05], loss: 3.2741809263825417e-11, sd:4.797539234161377\n",
      "Iteration: 16, w:2.99999737739563, g: [-5.2452087e-06], loss: 6.87805368215777e-12, sd:4.797539234161377\n",
      "Iteration: 17, w:2.9999988079071045, g: [-2.3841858e-06], loss: 1.4210854715202004e-12, sd:4.797539234161377\n",
      "Iteration: 18, w:2.999999523162842, g: [-9.536743e-07], loss: 2.2737367544323206e-13, sd:4.797539234161377\n",
      "Iteration: 19, w:2.999999761581421, g: [-4.7683716e-07], loss: 5.684341886080802e-14, sd:4.797539234161377\n"
     ]
    }
   ],
   "source": [
    "def test_adagrad_hand():\n",
    "    global losses, w, grad\n",
    "    # Second_derivation\n",
    "    sd = tf.Variable(0, dtype=tf.float32)\n",
    "    lr = tf.constant(0.6, dtype=tf.float32)\n",
    "    regular = 1e-8\n",
    "\n",
    "    update1 = tf.assign_add(sd, tf.square(grad[0]))\n",
    "    g_final = lr * grad[0] / (tf.sqrt(sd) + regular)\n",
    "    update2 = tf.assign(w, w - g_final)\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        print('--' * 20, 'Test adagrad hand', '--' * 20)\n",
    "        sess.run(init)\n",
    "        print(sess.run([grad, p, w], {x: 1}))\n",
    "        for i in range(20):\n",
    "            _ = sess.run([update1], feed_dict={x: 1})\n",
    "            w_, g_, l_, sd_ = sess.run([w, grad, losses, sd], feed_dict={x: 1})\n",
    "            print('Iteration: {}, w:{}, g: {}, loss: {}, sd:{}'.format(i, w_, g_, l_, sd_))\n",
    "            _ = sess.run([update2], feed_dict={x: 1})\n",
    "            \n",
    "test_adagrad_hand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "1. [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
