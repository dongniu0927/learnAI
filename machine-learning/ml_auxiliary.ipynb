{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Auxiliary\n",
    "\n",
    "使用ML需要的一些辅助知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 特征缩放\n",
    "在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。\n",
    "\n",
    "简单的方法：$x_i=\\dfrac{x_i-u_n}{s_n}$，其中$u_n$式平均值，$s_n$是标准差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 正则化 (Regularization)\n",
    "\n",
    "- 过拟合：**过于强调拟合原始数据**（如用高次多项式模型）, 在新数据上表现很差；\n",
    "- 欠拟合：**不能很好的适应训练集**（如用线性模型）；\n",
    "\n",
    "处理过拟合：\n",
    "\n",
    "- 丢弃一些特征；\n",
    "- **正则化**(保留所有的特征，但是**减少参数的大小**)；\n",
    "\n",
    "正则化思想：正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了。 所以我们要做的就是在一定程度上减小这些参数的值，这就是正则化的基本方法。\n",
    "\n",
    "对于高次多项式模型：$h_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2^2+\\theta_3x_3^3$，我们决定减少$\\theta_2$与$\\theta_3$的大小，方法就是: **修改代价函数，在这两个参数上设置一点惩罚**。\n",
    "\n",
    "如可以修改逻辑回归的代价函数为：  \n",
    "\n",
    "$J(\\theta) = \\dfrac{1}{2m}[\\sum_{i=1}^m (h_{\\theta}(x^{(i)})-y^{(i)} )^{2} + 1000\\theta_2^2 + 10000\\theta_3^2]$\n",
    "\n",
    "假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度, 这种方法的核心实际上在于：**都在原有算法更新规则的基础上令值$\\theta$减少一个额外的值**.\n",
    "\n",
    "$J(\\theta) = \\dfrac{1}{2m}[\\sum_{i=1}^m (h_{\\theta}(x^{(i)})-y^{(i)} )^{2} + \\lambda\\sum_{j=1}^n\\theta_j^2]$\n",
    "\n",
    "$\\theta_j := \\theta_j(1-\\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} $\n",
    "\n",
    "使用正则化需要注意:\n",
    "\n",
    "- 需要合理的调控正则化参数$\\lambda$的值：它太大，会欠拟合；它太小，会过拟合  \n",
    "- 我们不惩罚$\\theta_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1正则化与L2正则化\n",
    "\n",
    "L1正则化和L2正则化可以看做是对损失函数惩罚项的选择。上面为了避免过拟合就加入了L2正则化，它跟L1正则化的区别如下：\n",
    "\n",
    "- L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择（过滤掉一些不重要的特征）;\n",
    "- L2正则化可以防止模型过拟合, 一定程度上，L1也可以防止过拟合;\n",
    "\n",
    "**L1正则化**：假设带有L1正则化的损失函数为：$J=J_0+\\lambda\\sum_w|w|$，注意$J$是不完全可微的，令$L=\\lambda\\sum_w|w|$，此时我们的任务实际上变成了：**在$L$约束下求$J_0$的最小值**，当只考虑二维情况，即只有两个权值（$w^1, w^2$），此时$L=|w^1|+|w^2|$，L1正则化过程可以由下图展示：\n",
    "\n",
    "![norm_v1](./resources/ml_auxiliary3.jpg)\n",
    "\n",
    "上图中黑色是$L$函数的图形，等值线是$J_0$。当$J0$等值线与$L$图形首次相交的地方就是最优解。因为$L$函数有很多**突出的角**，J0与这些角接触的机率会远大于与L其它部位接触的机率，而在这些角上，会有很多权值等于0，这就是为什么L1正则化可以产生稀疏模型，进而可以用于特征选择。前面的$\\lambda$可以控制L图形的大小（反比关系）。\n",
    "\n",
    "**L2正则化**：同理上面L1正则化，其图形描述如下：\n",
    "\n",
    "![norm_v1](./resources/ml_auxiliary4.jpg)\n",
    "\n",
    "二维平面下L2正则化的函数图形是个圆，与方形相比，被磨去了棱角。因此J0与L相交时使得w1或w2等于零的机率小了许多，这就是为什么L2正则化不具有稀疏性的原因。\n",
    "\n",
    "> 范数部分可参考：[线性代数](../basics/linear_algebra.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.梯度检验\n",
    "当模型很复杂用梯度下降容易出现错误，可以考虑采用一种叫做梯度数值检验（Numerical Gradient Checking）的方法来检验计算的导数值是否正确。\n",
    "\n",
    "对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度:\n",
    "\n",
    "![%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20180915141246.png](./resources/ml_auxiliary2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.方差(Variance)与偏差(Bias)\n",
    "\n",
    "方差（Variance）是预测数据集本身的分散程度，偏差（Bias）是预测数据集偏离真实数据集的程度。你可以根据下图直观的了解到他们的意义与区别：\n",
    "\n",
    "<img src=\"./resources/ml_auxiliary1.png\" width=\"60%\" height=\"auto\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文档\n",
    "\n",
    "待补充..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
