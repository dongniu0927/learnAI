{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从线性回归到神经网络\n",
    "\n",
    "本节介绍从线性回归，感知机，逻辑回归，SVM等一直到神经网络的发展过程与这几种模型的关联与区别。\n",
    "\n",
    "我们知道函数的功能就是建立了特定的定义域范围内$X$的值与特定值域$Y$之间的关系。这种关系可以描述日常生活中特定观测现象与最终呈现结果之间的联系，当这种关系式被确定后，我们就可以说我们掌握了其中的规律，可以用这个函数来预测未知的观测。\n",
    "\n",
    "按照预测值是连续的还是离散的，我们可以对问题进行分类：**回归问题（连续值预测）**；**分类问题（离散值预测）**。\n",
    "\n",
    "对于构建一个机器学习模型，有三点最为核心：（1） **建立模型（如线性回归）**；（2）**构建损失函数（如交叉熵）**；（3）**选择算法优化损失函数（如梯度下降）**；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归（Linear Regression）\n",
    "\n",
    "- 模型：$y=WX+b$，其中$X$是一组输入特征，$W$是权值，$b$是偏置，这个模型其实就是拟合了一个超平面，预测就是在给定$X$的情况下预测超平面上某个点对应的函数值；\n",
    "- 损失函数：\n",
    "  - 最小二乘法（Least Square Method）：$min_{W,b}[\\sum_{i=1}^m ||y^{(i)} -\\hat{y}^{(i)} ||_2^{2}]$，使用这个损失函数亦可以称为最小二乘回归；\n",
    "  - 均方差（Mean Square Error）：$min_{W,b}[\\dfrac{1}{2m}\\sum_{i=1}^m (y^{(i)} -\\hat{y}^{(i)} )^{2}]$，这种用的更多；\n",
    "- 优化算法：\n",
    "  - 梯度下降：$\\theta_j=\\theta_j-\\alpha\\dfrac{\\partial}{\\partial\\theta_j}J(\\theta)$（for all $j$）；\n",
    "  - 正规方程：$\\theta =(X^T X)^{-1}X^{T}y$，只适合特征数目较少的线性模型；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他回归\n",
    "\n",
    "- 多项式回归（Polynomial Regression）：自变量指数大于1；\n",
    "- 岭回归（Ridge Regression）：岭回归是当数据遭受多重共线性（独立变量高度相关）时使用的一种技术，其就是在线性回归中加入了二阶正则化项；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类模型\n",
    "\n",
    "线性分类器：感知机、逻辑回归、线性SVM、Softmax分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机\n",
    "\n",
    "感知机的目的是尽可能全部正确分类。一个感知机就相当于神经网络中的一个神经元，我们可以构造一层感知机或者多层感知机，其就是神经网络的原型（只是激活函数与损失函数可能不一样）。\n",
    "\n",
    "- **模型**：$y=sign(WX+b)$，其在超平面上套了一个激活函数sign，sign(x)在x大于0取+1，小于0取-1，模型的$W$为超平面的法向量；\n",
    "- **损失函数**：我们用所有误分类点到超平面的距离和作为损失函数，已知某一点到超平面的距离计算方式$\\dfrac{1}{||W||}|WX_0+b|$，且$-y_i(WX_i+b)$必大于0，则可以去掉绝对值，得到损失函数为：$-\\dfrac{1}{||W||}\\sum_{X_i\\in M}y_i(WX_i+b)$，不考虑分母部分可以得到：$-\\sum_{X_i\\in M}y_i(WX_i+b)$。\n",
    "- **优化算法**：梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归\n",
    "\n",
    "其就是在线性回归的上面套了一个sigmoid函数（输出规范化到0与1之间）。其也可以看作神经网络中的一个神经元。\n",
    "\n",
    "- 模型：$y=Sigmoid(WX+b), \\ Sigmoid(z)=\\dfrac{1}{1+e^{-z}}$，当结果大于等于0.5，$y$取1；当结果小于0.5，$y$取0；\n",
    "- 损失函数：使用交叉熵$J(\\theta)=\\dfrac{1}{m}\\sum_{i=1}^m[-y^{(i)}log\\hat{y}^{(i)} - (1-y^{(i)})log(1-\\hat{y}^{(i)})]$；\n",
    "- 优化算法：梯度下降\n",
    "\n",
    "其跟感知机的区别在于：\n",
    "\n",
    "- 激活函数不一样。感知机使用sign；逻辑回归用sigmoid \n",
    "- 损失函数不一样。感知机最小化错误分类点到超平面的距离和；逻辑回归最小化交叉熵；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "实现多分类。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
