{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 信息论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息量\n",
    "\n",
    "我们讲的每一句话都是带有信息的（如说手机是黑色的），我们可以用某种方式来衡量其携带的信息量（可以认为产生这个量的本质是因为**世界的不确定性**）。**事件拥有的信息量与事件发生的概率有关系，越不可能发生的事情发生了，其携带的信息量就越大**。如有下面两个事件：\n",
    "\n",
    "- 事件A：巴西队进入了2018世界杯决赛圈。\n",
    "- 事件B：中国队进入了2018世界杯决赛圈。\n",
    "\n",
    "因为事件B发生的可能性更小，所以事件B携带的信息量相比于事件A更大。\n",
    "\n",
    "假设$X$是一个离散型随机变量，其取值集合为$χ$，概率分布函数$p(x)=P(X=x)$，则定义事件$X=x_0$的信息量为：\n",
    "\n",
    "> $I(x_0)=−log_2(p(x_0))$\n",
    "\n",
    "这里对数以2为底，对数函数的使用请参考[数学基础](./mathematics.ipynb)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息熵\n",
    "\n",
    "因为一般我们讲话都会偏向某一个结果，实际上其还可能是其他结果，这样我们可以估算所有结果的期望。**信息熵表示了对于某个随机变量所有信息量的期望**。它的值越大：\n",
    "\n",
    "- **表明我们需要更多的信息量才能确定系统；**\n",
    "- **换言之即表明系统的不确定越大（因为信息量越大，概率越小）；**\n",
    "\n",
    "熵可由下式计算：\n",
    "\n",
    "> $H(X)=-\\sum_{i=1}^np(x_i)log(p(x_i))$\n",
    "\n",
    "当针对0-1分布问题，熵的计算方式可以简化为：\n",
    "\n",
    "> $H(x)=-p(x)log(p(x))-(1-p(x))log(1-p(x))$。\n",
    "\n",
    "熵只依赖$X$的概率分布。下图展示了熵随着$P(X=1)$概率分布变化的变化过程（随机变量$X$的取值为$\\{0, 1\\}$）：\n",
    "\n",
    "![info_theory1](./resources/info_theory1.png)\n",
    "\n",
    "由上图可以知道，当$P(X=1)$与$P(X=0)$都等于0.5的时候，熵的值最大，即系统的不确定性也最大。\n",
    "\n",
    "**并不是某一个随机变量概率越小，熵就越大，应该是所有随机变量的取值平分概率1的时候，系统信息熵最大，即系统不确定性最大**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相对熵（KL散度）\n",
    "\n",
    "随机变量A有两个取值1,0。一个概率分布认为p(1)=1, p(0)=0；另一个概率分布认为p(1)=0.8，p(0)=0.2。\n",
    "\n",
    "如果对于同一个随机变量$X$有两个单独的概率分布$P(X)$（如分类任务中某一类的真实概率）和$Q(X)$（如分类任务中某一类的预测概率），我们可以使用KL散度（Kullback-Leibler divergence）来衡量这两个分布的差异，即：**如果用$P$来描述目标问题，而不是用$Q$来描述目标问题，得到的信息增量**，可用如下公式计算：\n",
    "\n",
    "> $D_{KL}(p||q)=\\sum_{i=1}^np(x_i)[logp(x_i)-logq(x_i)]$\n",
    "\n",
    "$D_{KL}$的值越小，表示q分布和p分布越接近（预测逼近真实）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉熵\n",
    "将KL散度的计算公式从减号分开，前面部分刚好是$p$分布的熵，后面部分就是交叉熵，计算方式如下：\n",
    "\n",
    "> $H(p, q)=-\\sum_{i=1}^np(x_i)log(q(x_i))$\n",
    "\n",
    "其中$p(x_i)$是真实分布概率，$q(x_i)$是预测分布的概率。在机器学习中，我们需要评估label和predict之间的差距，使用KL散度刚刚好，由于KL散度中的前一部分$−H(y)$不变，故在优化过程中，只需要关注交叉熵就可以了。\n",
    "\n",
    "KL散度表示了如果要让预测值携带的数据量等于实际信息量还需要的信息增量，其值越小越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "- [一文搞懂交叉熵在机器学习中的使用](https://blog.csdn.net/tsyccnh/article/details/79163834)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
