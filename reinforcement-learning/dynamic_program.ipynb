{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动态规划（Dynamic Programming）\n",
    "\n",
    "动态规划(Dynamic Programming) ，一种寻找最优策略的算法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 策略(Policy)\n",
    "\n",
    "策略告诉主体(agent)在当前的状态下，应该选择哪个行动。如果数据化这种说法，变成：策略告诉主体(agent)在每个状态s下，选择行动a的可能性。\n",
    "\n",
    "可以考虑建立一个状态-行为矩阵：每一行代表一个state，每一列代表一个action，单元的值是一个取值区间为$[0,1]$的小数，代表对应状态-行动的**选择概率**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最优策略(Optimal Policy)\n",
    "\n",
    "最优策略是可以取得最大的长期奖赏（$G_t$）的策略。要获得这个策略就要计算价值，有两个计算公式：\n",
    "\n",
    "- 一个是策略的状态价值公式，其利于发现哪个状态的价值高。也就是**找到最优状态**；\n",
    "- 一个是策略的行动价值公式，其有利于发现（在特定状态下）哪个行动的价值高。也就是**找到最优行动**；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通用策略迭代(Generalized Policy Iteration)\n",
    "\n",
    "使用一种基于动态规划的通用策略迭代方式：\n",
    "\n",
    "1. 先从一个策略$\\pi_0$开始\n",
    "2. **策略评估(Policy Evaluation)**，得到策略$\\pi_0$的价值$v_{\\pi_0}$\n",
    "3. **策略改善(Policy Improvement)** ，根据价值$v_{\\pi_0}$，优化策略为$\\pi_1$\n",
    "4. 迭代上步骤2和3，直到找到最优价值$v_∗$，因此可以得到最优策略$\\pi_∗$（终止条件：得到了稳定的策略$\\pi$和策略价值$v_\\pi$）\n",
    "\n",
    "其迭代过程可以形象化的表示为：\n",
    "\n",
    "![dynamic_program1.png](./resources/dynamic_program1.png)\n",
    "\n",
    "在这个过程中，最核心的两个问题是：\n",
    "\n",
    "- **如何计算策略的价值**\n",
    "- **如何根据策略价值来优化策略**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 策略迭代(Policy Iteration)的实现步骤\n",
    "\n",
    "步骤如下：\n",
    "\n",
    "- **初始化所有状态的价值**：所有状态可表示为集合$S=\\{s_0,s_1,...,s_n\\}$，初始化所有状态的价值，如都设为0，$V_0(S)=[0,0,...,0]$。\n",
    "- **初始化一个等概率随机策略$\\pi_0$** ：状态-动作概率矩阵中每个元素的值$\\pi(s, a)$都等于$\\dfrac{1}{N_s}$，其中$N_s$表示在s状态下可选择的动作总数，初始的时候，一个状态s对应的所有可能行动a，都是有值的。而**找到最优策略的过程就是优化状态-动作概率矩阵$\\pi$，减少每个状态s可选的行动a**。\n",
    "- **策略迭代-策略评估过程**：根据当前策略获得状态价值函数，这个函数的获得是基于贝尔曼迭代方程式：\n",
    "\n",
    "![dynamic_program2.png](./resources/dynamic_program2.png)\n",
    "\n",
    "- **策略迭代-策略优化过程**：根据状态价值函数$V_{k+1}(S)$来优化策略$\\pi_k$。优化方法就是对于每个状态$s$，只保留可达到最大状态价值的行动。如有四个actions：A(10),B(10),C(8),D(7)，括号里是状态价值，则只保留行动A与B。这是一个贪恋的策略(greedy policy)，因为只做了一步价值计算。\n",
    "\n",
    "迭代结束条件：**得到了稳定的策略$\\pi$和策略价值$v_\\pi$**。策略$\\pi$稳定，即$\\pi_{k+1}=\\pi_k$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "通用策略迭代(DPI)是强化学习的核心思想，影响了几乎所有的强化学习方法。其通用思想是：两个循环交互的过程，**迭代策略评估**和**迭代策略优化**。动态规划(DP)对复杂的问题来说，可能不具有可行性。主要原因是问题状态的数量很大，导致计算代价太大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "- [强化学习读书笔记 - 04 - 动态规划](https://www.cnblogs.com/steven-yang/p/6493328.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
