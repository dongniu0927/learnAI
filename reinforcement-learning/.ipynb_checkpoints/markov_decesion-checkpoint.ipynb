{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 马尔科夫决策过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 强化学习任务划分\n",
    "\n",
    "强化学习任务通常可分为：\n",
    "\n",
    "- **情节性任务(Episodic Tasks)**：所有的任务可以被可以分解成一系列情节。逻辑上，可以看作为有限步骤的任务。 \n",
    "- **连续任务(Continuing Tasks)**：所有的任务不能分解。可以看作为无限步骤任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 马尔科夫属性(The Markov property)\n",
    "\n",
    "通常一般会认为下一步的状态与奖励是前面所有步骤一起决定的：\n",
    "\n",
    "> $P(S_{t+1}=s',R_{t+1}=r|S_0,A_0,R_1,S_1,A_1,...,R_t,S_t,A_t)$\n",
    "\n",
    "但是马尔科夫属性假设：**下一个状态与奖励只由当前状态与选择的动作所决定**：\n",
    "\n",
    "> $P(S_{t+1}=s', R_{t+1}=r|S_t=s,A_t=a)$\n",
    "\n",
    "这样模型就被大大简化了，**我们使用马尔科夫决策过程来描述强化学习模型**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 马尔科夫决策过程 - 数学模型\n",
    "\n",
    "马尔科夫数学模型可以从以下几个视图来描述：\n",
    "\n",
    "- 状态(state)-行动(action)-奖赏(reward)视图\n",
    "- 目标(goal)-奖赏(reward)视图\n",
    "- 决策过程视图\n",
    "- 策略(policy)视图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 状态(state)-行动(action)-奖赏(reward)视图\n",
    "\n",
    "![mkv1](./resources/markov1.png)\n",
    "\n",
    "上图是一个基本的马尔科夫决策过程。\n",
    "\n",
    "描述Agent在状态$s$下，选择了行动$a$，状态变为$s′$，获得了奖赏$r$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目标(goal)-奖赏(reward)视图\n",
    "\n",
    "![mkv2](./resources/markov2.png)\n",
    "\n",
    "奖赏假设(reward hypothesis)的目标就是：**最大化长期累计奖赏的期望值**。\n",
    "\n",
    "第$t$步决策带来的长期奖励$G_t$:\n",
    "\n",
    "> $G_t = \\sum_{k=0}^T \\gamma^kR_{t+k+1}, \\ 0 \\le \\gamma \\le 1$\n",
    "\n",
    "其中折扣率$\\gamma$决定了未来奖赏的当前价值，取值$[0, 1]$。\n",
    "\n",
    "如$T$等于无穷大，称为连续任务(Continuing Tasks)；否则称为情节性任务(Episodic Tasks)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策过程视图\n",
    "\n",
    "![mkv3](./resources/markov3.png)\n",
    "\n",
    "根据上图可以发现：\n",
    "\n",
    "- 状态$s$下，采取行动$a$，转变成新状态$s′$，是由概率$p(s′|s,a)$决定的。\n",
    "- 状态$s$下，采取行动$a$，转变成新状态$s′$，获得的奖赏$r$，是由概率$p(s′,r|s,a)$决定的。\n",
    "- 引起状态$s$到状态$s′$的转变行动，不一定是唯一的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 策略视图\n",
    "\n",
    "强化学习的目标是找到（可以获得长期最优回报）的最佳策略。\n",
    "\n",
    "策略规定了状态$s$时，应该选择的行动$a$：$\\pi=[\\pi(s_1),\\pi(s_2),...\\pi(s_n)]$。\n",
    "\n",
    "$\\pi(a|s)$表示随机策略$\\pi$在状态$s$下，选择的行动$a$的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 两种价值评估方式\n",
    "\n",
    "- 状态价值：$v_\\pi(s)=E(G_t|S_t=s)=E_\\pi(\\sum_{k=0}\\gamma^kR_{t+k+1}|S_t=t)$\n",
    "- 行动价值：\n",
    "$q_\\pi(s, a)=E(G_t|S_t=s,A_t=a)=E_\\pi(\\sum_{k=0}\\gamma^kR_{t+k+1}|S_t=s,A_t=a)$\n",
    "- 迭代状态价值（**Bellman equation**）：\n",
    "\n",
    "![](./resources/markov4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最优价值方法\n",
    "\n",
    "- 最优价值：$v_*(s)=max_\\pi v_\\pi(s), \\ \\ s\\in S$\n",
    "- 最优行动：$q_*(s, a)=max_\\pi q_\\pi(s, a), \\ \\ s\\in S \\ and \\ a \\in A(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "- [强化学习读书笔记 - 03 - 有限马尔科夫决策过程](https://www.cnblogs.com/steven-yang/p/6480666.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
