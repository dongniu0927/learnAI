{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习数学符号\n",
    "\n",
    "本文包含了强化学习中常用的数学符号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本概念\n",
    "\n",
    "- $s$：一个表示环境状态(state)的数据。\n",
    "- $S$：环境中所有可能状态(state)的集合。\n",
    "- $a$：一个智能体可以做的动作(action)。\n",
    "- $A$：智能体可以做的所有动作的集合。\n",
    "- $A(s)$：智能体在状态s下，可以做的所有动作的集合。\n",
    "- $r$：智能体在一次行动后，获得的奖赏(reward)。\n",
    "- $R$：本体可以获得的所有奖赏集合。\n",
    "- $S_t$：第t步的状态(state)，t从0开始。\n",
    "- $A_t$ - 第t步的行动(action)，t从0开始。\n",
    "- $R_t$ - 第t步的奖赏(reward)，t从1开始。\n",
    "- $G_t$ - 第t步的**长期回报**，t从0开始。\n",
    "\n",
    "**强化学习的目标1：追求最大回报。** $G_t$可以根据下式计算：\n",
    "\n",
    "> $G_t=R_t+\\gamma R_{t+1}+\\gamma^2R_{t+2}+..., \\ 0 \\le \\gamma \\le 1$\n",
    "\n",
    "可以看出，当$\\gamma=0$时，只考虑当前的奖赏；当$\\gamma=1$时，未来的奖赏没有损失。使用$G_t^{(n)}$表示第t步的n步回报（n-step return），其可以由下式计算：\n",
    "\n",
    "> $G_t^{(n)}=\\sum_{k=0}^n\\gamma^kR_{t+k+1}$\n",
    "\n",
    "这里是强化学习的第一个重点：**长期回报**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 策略Policy\n",
    "\n",
    "使用$\\pi$表示策略，**强化学习的目标2：找到最优策略**。策略规定了状态s时，应该选择的行动a:\n",
    "\n",
    "> $\\pi=[\\pi(s_1), \\pi(s_2)...,\\pi(s_n)]$\n",
    "\n",
    "- $\\pi(s)$ ： 策略$\\pi$决定在状态$s$下，应该选择的行动。\n",
    "- $\\pi_*$：最优策略(optimal policy)，使用本策略可获得的长期回报最大。\n",
    "- $\\pi(a|s)$： **随机策略**$\\pi$在状态$s$下，选择的行动$a$的概率。\n",
    "- $r(s,a)$：在状态$s$下，选择行动$a$的奖赏。\n",
    "- $p(s′|s,a)$：状态$s$、行动$a$的前提下，变成状态$s’$的概率。\n",
    "- $v_{\\pi}(s)$：状态价值。使用策略$\\pi$，状态$s$的长期奖赏$G_t$。\n",
    "- $q_{\\pi}(s,a)$ ：行动价值。使用策略$\\pi$，状态$s$下选择行动$a$的长期奖赏$G_t$。\n",
    "- $v_∗(s)$ ： 最佳状态价值。\n",
    "- $q_∗(s,a)$： 最佳行动价值。\n",
    "- $V(s)$： $v_{\\pi}(s)$的集合。\n",
    "- $Q(s,a)$ ： $q_{\\pi}(s,a)$的集合。\n",
    "\n",
    "上面的$v_{\\pi}(s)$可以由下式计算：\n",
    "\n",
    "> $v_{\\pi}(s)=E_\\pi[G_t|S_t=s]=E_\\pi[\\sum_{k=0}^\\infty]\\gamma^kR_{t+k+1}|S_t=s]$\n",
    "\n",
    "上面的$q_{\\pi}(s,a)$可以由下式计算：\n",
    "\n",
    "> $q_{\\pi}(s, a)=E_\\pi[G_t|S_t=s, A_t=a]=E_\\pi[\\sum_{k=0}^\\infty]\\gamma^kR_{t+k+1}|S_t=s,A_t=a]$\n",
    "\n",
    "$v_{\\pi}(s)$与$q_{\\pi}(s,a)$可以通过下式建立关系：\n",
    "\n",
    "> $v_\\pi(s)=max \\ q_\\pi(s, a)$\n",
    "\n",
    "$\\pi(s)$的取值由下面两种式子决定：\n",
    "\n",
    "> $\\pi(s)=argmax_a v_\\pi(s'|s, a)$，$\\pi(s)$是**选择的一个可以让下一个状态拥有最大价值的动作**  \n",
    "> $\\pi(s)=argmax_a q_\\pi(s, a)$，$\\pi(s)$是**选择一个在当前状态可以获得最大价值的动作**\n",
    "\n",
    "从上面式子可以看出，$\\pi(s)$是由$v_\\pi(s)$或者$q_\\pi(s, a)$决定。**强化学习的目标3：找到最优价值函数$v_∗(s)$或者$q_*(s,a)$**。\n",
    "\n",
    "这里是强化学习的第二个重点：**策略**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考文献\n",
    "\n",
    "-  [强化学习读书笔记 - 00 - 术语和数学符号](http://www.cnblogs.com/steven-yang/p/6481772.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
