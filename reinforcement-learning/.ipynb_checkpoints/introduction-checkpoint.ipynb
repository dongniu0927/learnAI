{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习（reinforcement learning）\n",
    "\n",
    "**强化学习RL是机器学习算法中的一大分支**。它的本质是**解决Decision Making问题**。使用它可以让计算机从一开始什么都不懂，通过不断的与环境**交互**，**试错**，**累积经验**，**学习经验**，最终学习到一个最优策略，这个策略可以在与环境的交互中获得很高的奖励。\n",
    "\n",
    "RL包含四个主要元素：\n",
    "\n",
    "- **Agent**：智能体\n",
    "- **Environment**：智能体所处环境状态\n",
    "- **Action**：智能体每一步采取的动作\n",
    "- **Reward**：智能体每采取一个动作后环境的反馈\n",
    "\n",
    "RL的目标就是**获得最多的累计奖励**，其模型架构如下：\n",
    "\n",
    "![intro1](./resources/intro1.png)\n",
    "\n",
    "上图大脑就是智能体（Agent），地球是环境（Environment），在每个时刻$t$:\n",
    "\n",
    "- Agent要做的是：\n",
    "\n",
    "  - 观察环境状态$O_t$\n",
    "  - 计算环境反馈的奖励收益$R_t$\n",
    "  - 做出行动$A_t$\n",
    "\n",
    "- Environment要做的是：\n",
    "\n",
    "  - 感知Agent的$A_t$\n",
    "  - 做出环境反应$O_{t+1}$\n",
    "  - 给出反馈$R_{t+1}$\n",
    "\n",
    "RL中的主要问题包括是：\n",
    "\n",
    "- RL的奖励是**稀疏**与**延时**的，这带来的问题就是对于一次奖励如何合理地分配给之前的每一个动作，这被称为**信用分配问题（credit assignment problem）**。\n",
    "- 另一个问题在于Agent要学会在**探索（exploration）**和**开发（exploitation）**之间做权衡，找出可以获得最大回报的决策。所谓探索，就是尝试一些新的不一样的方法，看看能不能获得更高的回报；所谓开发，就是尝试利用过去经验中最有效的行为来决策。\n",
    "\n",
    "运用RL的例子有很多：\n",
    "\n",
    "- 如Alpha Go代表机器第一次在围棋上战胜人类。\n",
    "- 如让计算机学会自己玩Atari，Breakout, 星际争霸等游戏。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 强化学习与监督学习\n",
    "\n",
    "监督学习可以直接给出正确或错误的标签结果，强化学习没有确定的标签，它只给出一个环境反馈的奖励值，来说明这一步的好坏程度，且这个反馈可能还有**延时**。另一方面监督学习的输入很多时候是独立同分布的，而强化学习的决策是**呈链状**，每做出一个Action会影响下一次的Action的决定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 马尔科夫决策过程\n",
    "\n",
    "一般使用**马尔科夫决策过程**来形式化**表述**和**推导**RL模型。\n",
    "\n",
    "假设一个**个体(agent)**，处在**一个环境（environment）** 中（例如Breakout游戏）。这个环境在某一时刻$t$有一个确定的**状态（state ）**（例如球拍的位置，球的位置和角度，砖的数量和位置等等），个体可以做出一个特定的**动作（action）**（例如左移或者右移球拍）。这个动作会获得一个**奖励（reward）**（例如积分增加或积分减少），然后使环境转移到下一个状态，接着个体再次做出下一个动作，循环往复直到游戏结束。\n",
    "\n",
    "动作可能有很多个，我们选择动作的规则称为**策略（policy）**。环境可能是随机的，这意味着下一个状态可能是随机的（比如Breakout游戏丢失一个球后，新开的球的位置和方向都是随机的）。\n",
    "\n",
    "State, Action, Reward的转换构成了一个马尔科夫过程，其表示为一个序列：\n",
    "\n",
    "> $s_0, a_0, r_1,...,s_{n-1}, a_{n-1}, r_n, s_n$\n",
    "\n",
    "上面$s_i$表示第$i$步的环境状态，$a_i$表示Agent在第$i$步采取的动作，$r_{i+1}$表示执行这个动作后获得的环境反馈奖励。\n",
    "\n",
    "马尔科夫决策过程的假设是**下一个状态$s_{i+1}$只取决于当前的状态$s_i$及采取的动作$a_i$，而与之前的状态和动作无关**（以下棋为例，下一步该怎么落子，仅仅观察当前的棋局做出决定就可以了，不需要关心是怎么走到当前这个地步的）。\n",
    "\n",
    "### 奖励计算\n",
    "\n",
    "在使用马尔科夫决策过程表征RL模型的基础上，可以计算出整个决策过程的奖励：\n",
    "\n",
    "> $R=r_1+r_2+r_3...+r_n$\n",
    "\n",
    "$t$时刻做出$a_t$后，未来的奖励可以表示为：\n",
    "\n",
    "> $R_t=r_t+r_{t+1}+r_{t+2}+...+r_n$\n",
    "\n",
    "但是我们实际算的时候，考虑到当前的动作对越远的未来影响会越小，故可以乘上一个$(0,1)$之间的折扣因子$\\lambda$，最终$t$时刻的奖励可以表示为：\n",
    "\n",
    "> $R_t=r_t+\\lambda r_{t+1}+\\lambda^2r_{t+2}+...+\\lambda^{(n-t)}r_n$  \n",
    "> $R_t=r_t+\\lambda R_{t+1}$\n",
    "\n",
    "上面第二个式子是第一个式子的递推式子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文档\n",
    "\n",
    "- [一文了解强化学习](https://blog.csdn.net/aliceyangxi1987/article/details/73327378)\n",
    "- [揭开深度增强学习的神秘面纱](http://blog.sina.com.cn/s/blog_44befaf60102wh1p.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
