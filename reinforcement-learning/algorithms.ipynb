{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各种算法介绍\n",
    "\n",
    "寻找强化学习最优策略可分为如下几类方法：\n",
    "\n",
    "- **Policy based**: 关注点是找到最优策略。\n",
    "- **Value based**: 关注点是找到最优奖励总和。\n",
    "- **Action based**: 关注点是每一步的最优行动。\n",
    "\n",
    "算法可以按照如下方式分类：\n",
    "\n",
    "![](./resources/algorithms.png)\n",
    "\n",
    "- **Model-based与model-free**: 基于模型还是无模型是看(状态或者行动)价值 (G,v(s),q(s,a)) 是如何得到的：如果是已知的、根据已知的数据计算出来的，就是基于模型的；如果是采样得到的、试验得到的，就是无模型的。\n",
    "- **Policy-based**与**Value-policy**: 基于策略通过分析环境直接输出下一步要采取的各种动作的概率, 然后根据概率采取行动；基于价值输出的是所有动作的价值, 根据最高价值来选动作，这类方法不能选取连续的动作。\n",
    "- **Monte-carlo update**与**Temporal-difference update**:蒙特卡洛方法(MC)是完成一轮训练再更新网络；时分差方法(TD)是每一步(或几步)就更新以下，边玩边学习。\n",
    "- **On-policy**与**Off-policy**: 在线学习就是本人边玩边学习；离线学习就是可以自己玩，也可以使用别人玩的信息来学习。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
