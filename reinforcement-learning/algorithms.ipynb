{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各种算法介绍\n",
    "\n",
    "寻找强化学习最优策略可分为如下几类方法：\n",
    "\n",
    "- **Policy based**: 关注点是找到最优策略。\n",
    "- **Value based**: 关注点是找到最优奖励总和。\n",
    "- **Action based**: 关注点是每一步的最优行动。\n",
    "\n",
    "算法可以按照如下方式分类：\n",
    "\n",
    "![](./resources/algorithms.png)\n",
    "\n",
    "- **Model-based与model-free**: 基于模型还是无模型是看(状态或者行动)价值 (G,v(s),q(s,a)) 是如何得到的：如果是已知的、根据已知的数据计算出来的，就是基于模型的；如果是采样得到的、试验得到的，就是无模型的。\n",
    "- **Policy-based**与**Value-policy**: 基于策略通过分析环境直接输出下一步要采取的各种动作的概率, 然后根据概率采取行动；基于价值输出的是所有动作的价值, 根据最高价值来选动作，这类方法不能选取连续的动作。\n",
    "- **Monte-carlo update**与**Temporal-difference update**:蒙特卡洛方法(MC)是完成一轮训练再更新网络；时分差方法(TD)是每一步(或几步)就更新以下，边玩边学习。\n",
    "- **On-policy**与**Off-policy**: 在线学习就是本人边玩边学习；离线学习就是可以自己玩，也可以使用别人玩的信息来学习。\n",
    "\n",
    "其他分类说明：**使用表格学习的Q-learning与Sarsa；使用神经网络学习的Deep-Q-Network；直接输出行为的Policy gradients；想象环境并从中学习的Model-based RL。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Q-learning是一种通过记录行为价值(Q value)的方法来获得最优策略的算法，其算法过程如下：\n",
    "\n",
    "![](./resources/algorithms1.png)\n",
    "\n",
    "上面实际上就是一种**策略评估不断迭代更新**的过程。评估策略使用Q价值函数，并使用迭代不断更新Q函数达到逐步优化策略的目的。计算新Q值使用了贝尔曼迭代方程式（存疑）。某个状态下一个动作的价值根据其长期回报来计算。\n",
    "\n",
    "**认为第一个a是估计值，第二个a'是现实值**，所以用显示值减去实际值来更新Q值。\n",
    "\n",
    "**之所以说Q-learning是off-policy，就是现实Q的计算可以使用其他源的数据。**\n",
    "\n",
    "注意：在Q-Learning中a动作是实际采用了的，而a'动作并没有真实采用（故使用了max，只是做估计）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-action-reward-state'-action' (Sarsa)\n",
    "\n",
    "Sarsa与Q-Learning类似，不同之处在于：对于Q-Learning中a'，sarsa会在下一步真实采用这个动作a'。对比如下所示：\n",
    "\n",
    "![](./resources/algorithms2.png)\n",
    "\n",
    "Sarsa是在线学习，学习自己在做的事；Q-Learning则是说到不一定做到，也叫离线学习。但因为Q-Learning有一个Max，所以相比于Sarsa更勇敢，它始终都会选择最近的一条通往成功的道路, 不管这条路会有多危险。而 Sarsa 则是相当保守。\n",
    "\n",
    "Sarsa在当前state已经想好了对应的action，而且想好了下一个state'与action'；Q-learning还没有想好下一个action'。Sarse更新Q(s,a)的时候基于的是下一个Q(s_, a_)；Qlearning是基于maxQ(s_))。\n",
    "\n",
    "**之所以说Sarsa是on-policy就是选择的现实Q就是它下一步要做的行为。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa(lambda)\n",
    "\n",
    "Sarsa(lambda)里面的lambda就是更新获得当前奖励前多少步的Q值。上面无lambda的Sarsa就是Sarsa(0)，即单步更新；当lambda取1则更新的是获取到reward前的所有步；当lambda取(0,1)之间，取值越大，离宝藏越近的步更新力度越大。\n",
    "\n",
    "**单步更新与回合更新**：对于单步更新，虽然每一步都更新，但是可能这一次更新并没有奖励，只有到最后一步才得到了奖励，最后一步被标志为很好的动作，前面所有步都认为与最终奖励无关；回合更新则会考虑最终奖励要分给前面所有步。\n",
    "\n",
    "**Sarsa和Qlearning都是每次获取到reward, 只更新获取到reward的前一步。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Network（DQN）\n",
    "\n",
    "DQN融合了神经网络与Q-Learning。产生的原因是：Q-learning与Sarsa都用表格来存储state与action，但是很多问题很复杂(如下围棋)，没有办法用表格来存储所有的状态与行为。\n",
    "\n",
    "![](./resources/algorithms3.png)\n",
    "\n",
    "有两种神经网络：\n",
    "1. **输入state+action，直接得到动作的Q值；**\n",
    "2. **输入state，输出所有动作的Q值，然后选择最大值动作；**\n",
    "\n",
    "针对第二种神经网络的训练过程：通过NN预测出Q(s1, a1)与Q(s1, a2)，这个就是Q的估计（相当于读了一次Table）；选择Q估计中较大的动作执行来换取环境的奖励Reward并到达s2；使用NN预测Q(s2, a3)与Q(s2, a4)，找最大的Q值，这个Q值与s1的Q值的差作为网络的误差来训练网络。\n",
    "\n",
    "DQN两大杀器：**Experience replay** 和 **Fixed Q-targets**。\n",
    "\n",
    "- **Experience replay**：就是计算现实Q的时候不使用当前NN的参数，而是加载某一个过去历史经验的参数来产生现实Q值。随机抽取这种做法打乱了经历之间的相关性, 也使得神经网络更新更有效率；  \n",
    "- **Fixed Q-targets**：在 DQN 中使用到两个结构相同但参数不同的神经网络, 预测Q估计的神经网络具备最新的参数, 而预测Q现实的神经网络使用的参数则是很久以前的；\n",
    "\n",
    "未完待续..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "策略梯度不同于基于价值值选择动作的更新方法，它直接输出每个动作被选择的概率值，它相比于基于值的方法更适合在连续区间内挑选动作。\n",
    "\n",
    "如果使用一个神经网络来输出所有动作的概率，更新方式就看**选择的动作得到的奖励值，值大的就增加这个动作输出概率；反之就减小**。\n",
    "\n",
    "一种Policy gradient的算法就是基于整个回合更新，叫做reinforce，其算法步骤如下：\n",
    "\n",
    "![](./resources/algorithms4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic\n",
    "\n",
    "Actor前生是Policy Gradients，这能让它毫不费力地在连续动作中选取合适的动作；Critic的前生是Q-learning或者其他的以值为基础的学习法, 其能进行单步更新。\n",
    "\n",
    "Actor需要每一步的奖惩信息来调节各种动作的概率，但是需要走完一个Episode才行，这就让学习速度很慢。有没有什么办法计算出每一步的奖惩值？Critic是一个以值为基础的算法，我们可以尝试用它计算每一步的奖惩值。结合二者得到：**Actor基于概率选行为, Critic基于Actor的行为评判行为的得分, Actor根据Critic的评分修改选行为的概率**。\n",
    "\n",
    "![](./resources/algorithms5.png)\n",
    "\n",
    "AC存在的一个问题是：**每次都是在连续状态中更新参数, 每次参数更新前后都存在相关性, 导致神经网络只能片面的看待问题。**\n",
    "\n",
    "一些改进版本：\n",
    "\n",
    "- **Deep Deterministic Policy Gradient(DDPG)**： AC + DQN，Google DeepMind 提出的一种使用 Actor Critic 结构, 但是输出的不是行为的概率, 而是**具体的行为（所以叫Deterministic）**, 用于连续动作 (continuous action) 的预测. DDPG 结合了之前获得成功的 DQN 结构, 提高了 Actor Critic 的稳定性和收敛性。\n",
    "- **Asynchronous Advantage Actor-Critic (A3C)**：Google DeepMind提出的一种通过同时训练多对副本Actor-Critic（之间互不干扰），然后跟主Actor-Critic共享信息，主Actor-Critic综合考量所有副本AC经验来更新参数（主结构的参数更新受到副结构提交更新的不连续性干扰），这样更新的相关性被降低, 收敛性提高。\n",
    "- **Distributed Proximal Policy Optimization（DDPO）**: OpenAI 提出的一种解决 Policy Gradient 不好确定 Learning rate (或者 Step size) 的问题. 因为如果 step size 过大, 学出来的 Policy 会一直乱动, 不会收敛, 但如果 Step Size 太小, 对于完成训练, 我们会等到绝望. PPO 利用 New Policy 和 Old Policy 的比例, 限制了 New Policy 的更新幅度, 让 Policy Gradient 对稍微大点的 Step size 不那么敏感。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "\n",
    "- [莫烦-强化学习](https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
