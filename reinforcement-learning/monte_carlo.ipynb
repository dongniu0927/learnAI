{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛方法(Monte Carlo Methods)\n",
    "\n",
    "蒙特卡洛是一个赌城的名字。冯·诺依曼给这方法起了这个名字，增加其神秘性。蒙特卡洛方法是一个计算方法，相对于确定性的算法，蒙特卡洛方法是**基于抽样数据**来计算结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本思路\n",
    "\n",
    "其基本思路是：**模拟** -> **抽样** -> **估值**。\n",
    "\n",
    "### 示例\n",
    "\n",
    "比如：如何求$\\pi$的值。一个使用蒙特卡洛方法的经典例子如下：\n",
    "\n",
    "我们知道一个直径为1的圆的面积为$\\pi$。把这个圆放到一个边长为2的正方形（面积为4）中，圆的面积和正方形的面积比是：$c=\\dfrac{\\pi}{4}$。如果可以测量出这个比值c，那么$\\pi=c\\times4$。如何测量比值c呢？可以用飞镖去扎这个正方形。扎了许多次后，用圆内含的小孔数除以正方形含的小孔数可以近似的计算比值c。\n",
    "\n",
    "### 示例说明\n",
    "\n",
    "- 模拟：用飞镖去扎这个正方形为一次模拟。\n",
    "- 抽样：数圆内含的小孔数和正方形含的小孔数。\n",
    "- 估值：比值c = 圆内含的小孔数 / 正方形含的小孔数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods的使用条件\n",
    "\n",
    "- **环境是可模拟的**：在实际的应用中，模拟容易实现。相对的，了解环境的完整知识反而比较困难。由于环境可模拟，我们就可以抽样。\n",
    "- **只适合情节性任务(episodic tasks)** ：因为，需要抽样完成的结果，只适合有限步骤的情节性任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC在强化学习中的应用\n",
    "\n",
    "强化学习目的是找到最优策略。得到最优策略是通过计算$v_\\pi(s),q_\\pi(s,a)$来获得的，这就是一个**求值问题**。\n",
    "\n",
    "结合通用策略迭代(GPI)与蒙特卡洛方法：\n",
    "\n",
    "- 策略评估：\n",
    "    - 探索：选择一个状态（会迭代所有状态）。\n",
    "    - 模拟：使用当前策略π，进行一次模拟，从当前状态(s, a)到结束，随机产生一段情节(episode)。\n",
    "    - 抽样：获得这段情节上的每个状态(s, a)的回报G(s,a)，记录G(s,a)到集合Returns(s,a)。\n",
    "    - 估值：$q(s, a) = Returns(s, a)$的平均值（因为状态(s, a)可能会被多次选择，所以状态(s, a)有一组回报值）。\n",
    " \n",
    "- 策略优化: 使用新的行动价值$q(s,a)$优化策略$\\pi(s)$。\n",
    "\n",
    "持续探索（continual exploration）是蒙特卡洛方法的主题。蒙特卡洛方法需要**大量的迭代**，才能正确的找到最优策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一些概念\n",
    "\n",
    "- on-policy method: 评估和优化的策略和模拟的策略是同一个。\n",
    "- off-policy method: 评估和优化的策略和模拟的策略是不同的两个。有时候，模拟数据来源于其它处，比如：已有的数据，或者人工模拟等等。\n",
    "- target policy: 目标策略。off policy method中，需要优化的策略。\n",
    "- behavior policy: 行为策略。off policy method中，模拟数据来源的策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 蒙特卡洛方法和动态规划的区别\n",
    "\n",
    "1. **动态规划是基于模型的，而蒙特卡洛方法是无模型的**。\n",
    "\n",
    "基于模型(model-base)还是无模型(model-free)**是看(状态或者行动)价值$(G,v(s),q(s,a))$是如何得到的**：如果是已知的、根据已知的数据计算出来的，就是基于模型的；如果是采样得到的、试验得到的，就是无模型的。\n",
    "\n",
    "2. **动态规划是引导性计算，而蒙特卡洛方法的计算是采样性的(sampling)**。\n",
    "\n",
    "引导性的(bootstrapping)还是采样性的(sampling)**是看(状态或者行动)价值$(G,v(s),q(s,a))$是如何计算的**：如果是根据其它的价值计算的，就是引导性的；如果是通过在实际环境中模拟的、采样的，就是采样性的。引导性和取样性并不是对立的。可以是采样的，并且是引导的。\n",
    "\n",
    "动态规划是从初始状态开始，一次计算一步可能发生的所有状态价值，然后迭代计算下一步的所有状态价值，这就是引导性。蒙特卡洛方法是从初始状态开始，通过在实际环境中模拟，得到一段情节（从头到结束）来迭代更新，所以使用MC是**单步更新的**。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
